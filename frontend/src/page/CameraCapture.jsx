import React, { useEffect, useRef, useState } from "react";
import { useNavigate } from "react-router-dom";
import * as faceapi from "face-api.js";

export default function CameraCapture() {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const overlayCanvasRef = useRef(null);
  const [state, setState] = useState({
    captured: false,
    fadeOut: false,
    fadeIn: 0,
    showConfirmation: false,
    showConsent: localStorage.getItem("termsAccepted") !== "true",
    consentGiven: false,
    lighting: "Analyzing...",
    facePosition: "No Face",
    modelsLoaded: false,
    error: null,
    uploadedImage: null
  });
  const navigate = useNavigate();
  const detectionInterval = useRef();
  // for holding the frozen frame
const [capturedDataUrl, setCapturedDataUrl] = useState(null);


  useEffect(() => {
    const timer = setTimeout(() => {
      setState(prev => ({ ...prev, fadeIn: 1 }));
    }, 100);
    return () => clearTimeout(timer);
  }, []);

  useEffect(() => {
    initializeCamera();
    return () => cleanup();
  }, []);

  const initializeCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { facingMode: "user" } 
      });
      videoRef.current.srcObject = stream;
      await loadModels();
    } catch (error) {
      handleError("Camera initialization failed:", error);
    }
  };

  const loadModels = async () => {
    try {
      await Promise.all([
        faceapi.nets.ssdMobilenetv1.loadFromUri(process.env.PUBLIC_URL + '/models'),
        faceapi.nets.faceLandmark68Net.loadFromUri(process.env.PUBLIC_URL + '/models'),
        faceapi.nets.faceExpressionNet.loadFromUri(process.env.PUBLIC_URL + '/models')
      ]);
      setState(prev => ({ ...prev, modelsLoaded: true }));
      startDetection();
    } catch (error) {
      handleError("Model loading failed. Check models in /public/models", error);
    }
  };

  const startDetection = () => {
    detectionInterval.current = setInterval(async () => {
      if (!state.captured && videoRef.current?.readyState >= 4) {
        try {
          const detections = await faceapi.detectAllFaces(
            videoRef.current,
            new faceapi.SsdMobilenetv1Options({ 
              minConfidence: 0.5,
              inputSize: 320 
            })
          );
          const { isValid, facePosition } = checkConditions(detections);
          updateOverlay(detections, isValid);
          setState(prev => ({
            ...prev,
            lighting: calculateLighting(),
            facePosition,
            facePresent: isValid
          }));
        } catch (error) {
          console.error("Detection error:", error);
        }
      }
    }, 100);
  };

  const updateOverlay = (detections, isValid) => {
    const canvas = overlayCanvasRef.current;
    const video = videoRef.current;
    if (!video?.videoWidth) return;

    const ctx = canvas.getContext("2d");
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.save();
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);

    const centerX = canvas.width / 2;
    const centerY = canvas.height / 2;
    const radius = Math.min(canvas.width, canvas.height) * 0.4;
    
    ctx.beginPath();
    ctx.arc(centerX, centerY, radius, 0, 2 * Math.PI);
    ctx.strokeStyle = "rgba(255, 255, 255, 0.6)";
    ctx.lineWidth = 4;
    ctx.stroke();

    detections.forEach(detection => {
      const box = detection.box;
      ctx.strokeStyle = isValid ? "rgba(0, 255, 0, 0.5)" : "rgba(255, 0, 0, 0.5)";
      ctx.lineWidth = 4;
      ctx.strokeRect(box.x, box.y, box.width, box.height);
    });

    ctx.restore();
  };

  const checkConditions = (detections) => {
    const video = videoRef.current;
    if (!video) return { isValid: false, facePosition: "No Face" };

    const videoWidth = video.videoWidth;
    const videoHeight = video.videoHeight;

    if (detections.length === 1) {
      const face = detections[0].box;
      const centerX = videoWidth / 2;
      const centerY = videoHeight / 2;
      const radius = Math.min(videoWidth, videoHeight) * 0.35;

      const faceCenterX = face.x + face.width / 2;
      const faceCenterY = face.y + face.height / 2;
      const buffer = 10;

      const distance = Math.sqrt(
        Math.pow(faceCenterX - centerX, 2) + 
        Math.pow(faceCenterY - centerY, 2)
      );
      const isCentered = distance <= radius + buffer;

      const targetDiameter = radius * 2;
      const minSize = targetDiameter * 0.6;
      const maxSize = targetDiameter * 0.9;
      const widthValid = face.width >= minSize && face.width <= maxSize;
      const heightValid = face.height >= minSize && face.height <= maxSize;

      const padding = Math.max(videoWidth * 0.05, 50);
      const withinFrame = face.x > padding && 
                         face.x + face.width < videoWidth - padding &&
                         face.y > padding && 
                         face.y + face.height < videoHeight - padding;

      const isValid = isCentered && widthValid && heightValid && withinFrame;
      
      return {
        isValid,
        facePosition: isValid ? "Good" : "Adjust"
      };
    }
    return {
      isValid: false,
      facePosition: detections.length > 1 ? "Multiple faces" : "No Face"
    };
  };

  const calculateLighting = () => {
    const video = videoRef.current;
    // ‚Üê guard against zero dimensions
    if (!video.videoWidth || !video.videoHeight) {
      return "Analyzing‚Ä¶";
    }
  
    const canvas = document.createElement("canvas");
    const ctx = canvas.getContext("2d");
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
  
    // flip & draw
    ctx.save();
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    ctx.restore();
  
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
     let brightnessSum = 0;

    for (let i = 0; i < imageData.data.length; i += 4) {
      brightnessSum += (imageData.data[i] + imageData.data[i+1] + imageData.data[i+2]) / 3;
    }
    
    const avgBrightness = brightnessSum / (imageData.data.length / 4);
    if (avgBrightness < 80) return "Too Dark";
    if (avgBrightness > 180) return "Too Bright";
    return "Good";
  };
  
  const captureImage = () => {
    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext("2d");
  
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.save();
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    ctx.restore();
  
    // grab a lossless PNG data-URL
    const imageData = canvas.toDataURL("image/png");
  
    // üî¥ stop all camera tracks
    video.srcObject.getTracks().forEach((t) => t.stop());
  
    // store and flip our UI into ‚Äúcaptured‚Äù mode
    setCapturedDataUrl(imageData);
    setState((prev) => ({ ...prev, captured: true, showConfirmation: true }));
  };
  

  const handleUpload = (e) => {
    const file = e.target.files[0];
    if (!file) return;

    const reader = new FileReader();
    reader.onload = (event) => {
      const img = new Image();
      img.onload = async () => {
        const canvas = canvasRef.current;
        const ctx = canvas.getContext("2d");
        
        // Set canvas dimensions to match image dimensions
        canvas.width = img.width;
        canvas.height = img.height;

        // Draw the uploaded image
        ctx.imageSmoothingQuality = "high";
        ctx.drawImage(img, 0, 0, canvas.width, canvas.height);

        // Store the uploaded image
        const imageData = canvas.toDataURL('image/png');
        localStorage.setItem('image', imageData);

        // Update state to show confirmation
        setState(prev => ({
          ...prev,
          captured: true,
          showConfirmation: true,
          facePosition: "Uploaded Image",
          lighting: "Good"
        }));

        // Ensure models are loaded before detection
        if (!state.modelsLoaded) {
          try {
            await loadModels();
          } catch (error) {
            handleError("Failed to load models:", error);
            return;
          }
        }

        // Perform face detection on the uploaded image
        try {
          // Detect faces in the uploaded image
          const detections = await faceapi.detectAllFaces(canvas, new faceapi.SsdMobilenetv1Options({ 
            minConfidence: 0.5,
            inputSize: 320 
          }));

          // Calculate lighting for the uploaded image
          const lighting = calculateLightingForUploaded(canvas);

          // Check face position conditions
          const { isValid, facePosition } = checkConditionsForUploaded(detections, canvas);

          // Update state with analysis results
          setState(prev => ({
            ...prev,
            lighting,
            facePosition,
            facePresent: isValid
          }));
        } catch (error) {
          console.error("Error analyzing uploaded image:", error);
        }
      };
      img.src = event.target.result;
    };
    reader.readAsDataURL(file);
  };

  // Helper function to calculate lighting for uploaded image
  const calculateLightingForUploaded = (canvas) => {
    const ctx = canvas.getContext("2d");
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    let brightnessSum = 0;

    for (let i = 0; i < imageData.data.length; i += 4) {
      brightnessSum += (imageData.data[i] + imageData.data[i+1] + imageData.data[i+2]) / 3;
    }
    
    const avgBrightness = brightnessSum / (imageData.data.length / 4);
    if (avgBrightness < 80) return "Too Dark";
    if (avgBrightness > 180) return "Too Bright";
    return "Good";
  };

  // Helper function to check face position conditions for uploaded image
  const checkConditionsForUploaded = (detections, canvas) => {
    if (detections.length === 1) {
      const face = detections[0].box;
      const videoWidth = canvas.width;
      const videoHeight = canvas.height;

      const centerX = videoWidth / 2;
      const centerY = videoHeight / 2;
      const radius = Math.min(videoWidth, videoHeight) * 0.35;

      const faceCenterX = face.x + face.width / 2;
      const faceCenterY = face.y + face.height / 2;
      const buffer = 10;

      const distance = Math.sqrt(
        Math.pow(faceCenterX - centerX, 2) + 
        Math.pow(faceCenterY - centerY, 2)
      );
      const isCentered = distance <= radius + buffer;

      const targetDiameter = radius * 2;
      const minSize = targetDiameter * 0.6;
      const maxSize = targetDiameter * 0.9;
      const widthValid = face.width >= minSize && face.width <= maxSize;
      const heightValid = face.height >= minSize && face.height <= maxSize;

      const padding = Math.max(videoWidth * 0.05, 50);
      const withinFrame = face.x > padding && 
                         face.x + face.width < videoWidth - padding &&
                         face.y > padding && 
                         face.y + face.height < videoHeight - padding;

      const isValid = isCentered && widthValid && heightValid && withinFrame;
      
      return {
        isValid,
        facePosition: isValid ? "Good" : "Adjust"
      };
    }
    return {
      isValid: false,
      facePosition: detections.length > 1 ? "Multiple faces" : "No Face"
    };
  };

  const handleConsent = () => {
    localStorage.setItem("termsAccepted", "true");
    setState(prev => ({ ...prev, showConsent: false }));
  };

  const retake = () => {
    setState(prev => ({ ...prev, captured: false, showConfirmation: false }));
    // Clear the canvas
    const canvas = canvasRef.current;
    const ctx = canvas.getContext("2d");
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    // Resume video
    if (videoRef.current) {
      videoRef.current.play();
    }
  };

  const proceed = () => {
    setState(prev => ({ ...prev, fadeOut: true }));
    setTimeout(() => navigate("/loading"), 1000);
  };

  const handleError = (message, error) => {
    console.error(message, error);
    setState(prev => ({ ...prev, error: error.message, modelsLoaded: false }));
  };

  const cleanup = () => {
    clearInterval(detectionInterval.current);
    videoRef.current?.srcObject?.getTracks().forEach(track => track.stop());
  };

  return (
    <div className={`min-h-screen flex flex-col items-center justify-center relative px-5 py-16 mt-12`}
         style={{ 
           transition: "opacity 1s ease-in-out", 
           opacity: state.fadeOut ? 0 : state.fadeIn
         }}>
      
      <div className="absolute inset-0 bg-cover bg-center blur-lg"
           style={{ backgroundImage: "url('/assets/bg5.png')" }}></div>

      <h1 className="font-fanwood text-3xl sm:text-4xl text-darkblue mb-4 z-10 relative">
        Clare Analysis Model
      </h1>

      <p className="font-lato font-light italic text-sm sm:text-base text-gray-500 text-center max-w-3xl mb-6 z-10 relative">
        Our Clare Analysis Model utilizes cutting-edge AI technology to analyze
        your skin and generate a detailed report with personalized insights.
      </p>

      <div className="relative w-full max-w-md mt-8 z-10">
        
      {capturedDataUrl ? (
    // once captured, show the frozen frame
    <img
      src={capturedDataUrl}
      alt="Captured frame"
      className="transform scale-x-[-1] w-full h-[60vh] max-w-md rounded-3xl shadow-lg object-cover"
    />
  ) : (
    <>
      {/* live video + overlay */}
      <video
        ref={videoRef}
        autoPlay
        playsInline
        className="transform scale-x-[-1] w-full h-[60vh] max-w-md rounded-3xl shadow-lg object-cover"
      />
      <canvas
        ref={overlayCanvasRef}
        className="absolute top-0 left-1/2 transform -translate-x-1/2 w-full h-full pointer-events-none"
        style={{ zIndex: 10 }}
      />
    </>
  )}

        {state.showConsent && (
          <div className="absolute inset-0 flex items-center justify-center bg-black bg-opacity-50 rounded-3xl overflow-hidden">
            <div className="bg-white p-6 rounded-xl shadow-lg text-center w-full max-h-[90vh] overflow-y-auto">
              <h2 className="font-lato text-lg font-normal text-darkblue mb-4">
                Biometric Data Processing Consent
              </h2>
              
              <div className="font-lato font-light text-xs text-gray-700 text-left space-y-4">
                <p>
                  The Clare Skin Analysis Service ("Clare") may collect, process, 
                  and store biometric identifiers including facial geometry, 
                  skin characteristics, and related physiological data 
                  ("Biometric Data") through our AI-powered diagnostic platform.
                </p>

                <p>
                  <strong>Purpose of Collection:</strong> Your Biometric Data will be 
                  used exclusively to generate personalized skin health analysis reports, 
                  provide AI-driven treatment recommendations, improve diagnostic 
                  algorithms through secure processes, and maintain health records 
                  for your clinical history.
                </p>

                <p>
                  <strong>Data Management:</strong> All biometric information will be 
                  encrypted during storage and transmission, retained for a maximum 
                  period of 24 months from last access, and anonymized for 
                  research and development purposes.
                </p>

                <p>
                  Your consent is governed by our{" "}
                  <button 
                    onClick={() => navigate('/privacy')}
                    className="text-blue-600 underline mx-1"
                  >
                    Privacy Policy
                  </button> 
                  and{" "}
                  <button
                    onClick={() => navigate('/terms')}
                    className="text-blue-600 underline mx-1"
                  >
                    Terms of Service
                  </button>, 
                  which outline your rights under applicable data protection regulations.
                </p>
              </div>

              <div className="flex items-start mt-4 mb-2">
                <input
                  type="checkbox"
                  id="biometricConsent"
                  onChange={(e) => setState(prev => ({ ...prev, consentGiven: e.target.checked }))}
                  className="mt-1 mr-3"
                />
                <label htmlFor="biometricConsent" className="font-lato text-xs text-gray-700 text-left">
                  I hereby explicitly authorize Clare to process my Biometric Data 
                  as described above. I affirm that this consent is voluntary and 
                  informed, understanding that service access requires data processing 
                  and that I may withdraw consent through account deletion.
                </label>
              </div>

              <button
                onClick={handleConsent}
                disabled={!state.consentGiven}
                className={`font-lato font-light text-sm py-2 px-6 rounded-full mt-4 transition-colors ${
                  state.consentGiven
                    ? "bg-darkblue text-white hover:bg-opacity-80"
                    : "bg-gray-300 text-gray-500 cursor-not-allowed"
                }`}
              >
                Accept and Continue
              </button>
            </div>
          </div>
        )}

        {!state.showConsent && (
          <div className="absolute bottom-4 left-1/2 transform -translate-x-1/2 text-center bg-black bg-opacity-50 px-4 py-2 rounded-lg z-20">
            <p className="font-lato text-sm text-white">
              Lighting: {state.lighting} | 
              Position: {state.facePosition}
            </p>
          </div>
        )}
      </div>

      <div className="absolute left-0 right-0 flex justify-center space-x-4 p-2 z-20" style={{ top: 182 }}>
        <span className="font-lato font-light text-[#14213D]">Use our camera</span>
        <span className="font-lato font-light text-[#14213D] mx-2">or</span>
        <label className="font-lato font-light text-[#14213D] cursor-pointer underline">
          <input 
            type="file" 
            accept="image/*" 
            onChange={handleUpload} 
            className="hidden" 
          />
          Upload your own
        </label>
      </div>

      {!state.showConsent && (state.showConfirmation ? (
        <div className="mt-8 z-10 relative">
          <p className="font-lato font-light text-lg text-darkblue mb-4">
            Do you want to proceed with this image?
          </p>
          <div className="flex gap-4">
            <button
              onClick={retake}
              className="font-lato text-lg font-light bg-[#ff4d4d] text-white py-3 px-12 rounded-full transition-all duration-300 hover:opacity-80"
            >
              Retake
            </button>
            <button
              onClick={proceed}
              className="font-lato text-lg font-light bg-[#14213D] text-white py-3 px-12 rounded-full transition-colors duration-300 hover:opacity-80"
            >
              Proceed
            </button>
          </div>
        </div>
      ) : (
        <button
          onClick={captureImage}
          className="font-lato text-sm sm:text-lg font-light bg-[#14213D] text-white py-3 px-12 rounded-full transition-colors duration-300 hover:opacity-80 mt-8 z-10 relative"
          disabled={!state.facePresent || state.lighting !== "Good" || !state.modelsLoaded}
        >
          {state.modelsLoaded ? "Capture Image" : "Loading Models..."}
        </button>
      ))}

      {state.error && (
        <div className="font-lato font-light text-red-500 mt-4 z-10">
          Error: {state.error}. Please refresh the page.
        </div>
      )}
    </div>
  );
}

